{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of an AE + VAE\n",
    "\n",
    "Example of VAE and AE on MNIST dataset using a CNN.\n",
    "There are three models the encoder, decoder and autoencoder.\n",
    "The encoder generates the latent variables and the \n",
    "decoder can be used to generate MNIST digits by sampling. The VAE\n",
    "restricts the latent vector/variable to be from a Gaussian distribution \n",
    "with mean = 0 and std = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE Example\n",
    "\n",
    "- Here the xample uses Convolutional and MaxPooling layers in the enocder.\n",
    "- Uses Convolutional Transpose and UpSampling layers in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Load the module we need\n",
    "# Note that we are import the Keras backend, which is assumed to be Tensorflow\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D\n",
    "from tensorflow.keras.layers import Layer, UpSampling2D, Reshape, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "#from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.losses import MeanSquaredError, binary_crossentropy, mse, KLDivergence\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
    "    # Arguments\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent subspace\n",
    "    if (model_name == 'vae_mlp'):\n",
    "        z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    else:\n",
    "        z_mean = encoder.predict(x_test,batch_size=batch_size)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(np.amin(z_mean[:,0]), np.amax(z_mean[:,0]), n)\n",
    "    grid_y = np.linspace(np.amin(z_mean[:,1]), np.amax(z_mean[:,1]), n)[::-1]\n",
    "\n",
    "    z_sample = np.zeros((1,5))\n",
    "    z_sample[0,2] = np.mean(z_mean[:,2])\n",
    "    z_sample[0,3] = np.mean(z_mean[:,3])\n",
    "    z_sample[0,4] = np.mean(z_mean[:,4])    \n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample[0,0] = xi\n",
    "            z_sample[0,1] = yi\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = (n - 1) * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST datasets (training and test)\n",
    "batch_size = 128\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) \n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder, inputs are the MNIST 28x28 greyscale images \n",
    "input_img = Input(shape=(28, 28, 1),batch_size=batch_size)  \n",
    "\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu',padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu')(x)\n",
    "x = Reshape((128,))(x)\n",
    "z = Dense(5,activation='linear')(x)    \n",
    "\n",
    "encoded = Model(input_img,z,name='encoded')\n",
    "encoded.summary()\n",
    "plot_model(encoded, to_file='AE_encoded.png', show_shapes=True)            \n",
    "\n",
    "# Latent space is a 5 dimensional vector space\n",
    "\n",
    "# This is the decoder, it take the 5 dim latent vector and generates a 28x28 image\n",
    "lats = Input(shape=(5,),name='latent')\n",
    "x = Dense(128,activation='relu')(lats)\n",
    "x = Reshape((4,4,8))(x)\n",
    "x = Conv2DTranspose(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2DTranspose(32, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "rec = Conv2DTranspose(1, (3, 3), activation='relu',padding=\"same\")(x)\n",
    "\n",
    "decoded = Model(lats,rec,name='decoded')\n",
    "decoded.summary()\n",
    "plot_model(decoded, to_file='AE_decoded.png', show_shapes=True)     \n",
    "\n",
    "# The autoencoder takes the image as input encodes it, then decodes it\n",
    "rec = decoded(encoded(input_img))\n",
    "autoencoder = Model(input_img,rec)\n",
    "\n",
    "\n",
    "\n",
    "class ReconLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, name=\"ReconstructionLoss\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "\n",
    "        loss = tf.reduce_sum(tf.math.square(y_true - y_pred),axis=(1,2))\n",
    "        return tf.math.reduce_mean(loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config}\n",
    "\n",
    "loss=ReconLoss()\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta',loss=loss)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, this can take some time\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=10,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample of test images that have been through the autoencoder\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n + 1)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plot_results function to show a latent subspace \n",
    "# Use a grid of latent variables to generate MNIST images\n",
    "batch_size=128\n",
    "plot_results((encoded,decoded),\n",
    "            (x_test,y_test),\n",
    "                 batch_size=batch_size,\n",
    "                 model_name=\"AE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE\n",
    "\n",
    "- Here is an example of a Variational AutoEncoder.\n",
    "- The example wa changed extensively from gthe original, due to updates in tensorflow. \n",
    "- [The examnple followed cae from this link.](https://www.tensorflow.org/tutorials/generative/cvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "  \"\"\"Convolutional variational autoencoder.\"\"\"\n",
    "\n",
    "  def __init__(self, latent_dim):\n",
    "    super(CVAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.encoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "            Conv2D(\n",
    "                filters=32, kernel_size=3, strides=(1, 1), activation='relu', padding='same'),\n",
    "            MaxPooling2D(pool_size=2),\n",
    "            Conv2D(\n",
    "                filters=16, kernel_size=3, strides=(1, 1), activation='relu'),\n",
    "            MaxPooling2D(pool_size=2),\n",
    "            Conv2D(\n",
    "                filters=8, kernel_size=3, strides=(1, 1), activation='relu'),\n",
    "            Flatten(),\n",
    "            # No activation\n",
    "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    self.decoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(units=128, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Reshape(target_shape=(4, 4, 8)),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=16, kernel_size=3, strides=1, activation='relu'),\n",
    "            UpSampling2D(size=2),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=32, kernel_size=3, strides=1, activation='relu'),\n",
    "            UpSampling2D(size=2),            \n",
    "            # No activation\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  @tf.function\n",
    "  def sample(self, eps=None):\n",
    "    if eps is None:\n",
    "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "    return self.decode(eps)\n",
    "\n",
    "  def encode(self, x):\n",
    "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "    return mean, logvar\n",
    "\n",
    "  def reparameterize(self, mean, logvar):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "  def decode(self, z):\n",
    "    logits = self.decoder(z)\n",
    "    return logits\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "\n",
    "def KLnorm(mean, logvar, raxis=1):\n",
    "  return -0.5*tf.reduce_sum(\n",
    "        (1.0 + logvar - mean**2 - tf.math.exp(logvar)),\n",
    "        axis=raxis)\n",
    "\n",
    "def compute_loss(model, x):\n",
    "  mean, logvar = model.encode(x)\n",
    "  z = model.reparameterize(mean, logvar)\n",
    "  x_pred = model.decode(z)\n",
    "  recon = tf.math.square(x - x_pred)\n",
    "  rec_loss = tf.reduce_sum(recon, axis=[1, 2, 3])\n",
    "  kldiv = KLnorm(mean, logvar)\n",
    "  return tf.reduce_mean(rec_loss + kldiv)\n",
    "\n",
    "def compute_kldiv(model, x):\n",
    "  mean, logvar = model.encode(x)\n",
    "  z = model.reparameterize(mean, logvar)\n",
    "  kldiv = KLnorm(mean, logvar)\n",
    "  return tf.reduce_mean(kldiv)\n",
    "\n",
    "def compute_recon(model,x):\n",
    "  mean, logvar = model.encode(x)\n",
    "  z = model.reparameterize(mean, logvar)\n",
    "  x_pred = model.decode(z)\n",
    "  recon = tf.math.square(x - x_pred)\n",
    "  rec_loss = tf.reduce_sum(recon, axis=[1, 2, 3])\n",
    "  return tf.reduce_mean(rec_loss)\n",
    "\n",
    "def predict(model,x):\n",
    "  mean, logvar = model.encode(x)\n",
    "  x_pred = model.decode(mean)\n",
    "  return x_pred\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "  \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "  This function computes the loss and gradients, and uses the latter to\n",
    "    \n",
    "  update the model's parameters.\n",
    "  \"\"\"\n",
    "  with tf.GradientTape() as tape:\n",
    "      loss = compute_loss(model, x)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "train_size=60000\n",
    "test_size=10000\n",
    "batch_size = 128\n",
    "latent_dim = 5\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices(x_train)\n",
    "                 .shuffle(train_size).batch(batch_size))\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(x_test)\n",
    "                .shuffle(test_size).batch(batch_size))\n",
    "\n",
    "vae = CVAE(latent_dim)\n",
    "\n",
    "vae.encoder.summary()\n",
    "vae.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "# Train over batches\n",
    "  for train_x in train_dataset:\n",
    "    train_step(vae, train_x, optimizer)\n",
    "\n",
    "# Training loss at epoch\n",
    "  train_loss = 0\n",
    "  train_rec = 0\n",
    "  train_kl = 0\n",
    "  for train_x in train_dataset:\n",
    "    train_loss = train_loss + compute_loss(vae, train_x)\n",
    "    train_rec = train_rec + compute_recon(vae, train_x)\n",
    "    train_kl = compute_kldiv(vae, train_x)\n",
    "  train_loss = batch_size*train_loss/train_size\n",
    "  train_rec = batch_size*train_rec/train_size\n",
    "  train_kl = batch_size*train_kl/train_size\n",
    "\n",
    "# Test loss at epoch\n",
    "  test_loss = 0\n",
    "  for test_x in test_dataset:\n",
    "    test_loss = test_loss + compute_loss(vae, test_x)\n",
    "    test_kl = compute_kldiv(vae, test_x)\n",
    "  test_loss = batch_size*test_loss/test_size\n",
    "  print('Epoch: {}, Training loss: {}, Test loss: {}, Train Recon: {}, Train KL_div: {}'\n",
    "        .format(epoch,train_loss,test_loss,train_rec,train_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some reconstruction examples from the test set\n",
    "\n",
    "for train_x in train_dataset.take(batch_size):\n",
    "    decoded_imgs = predict(vae,train_x).numpy()\n",
    "\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n + 1)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the latent subspace and generate MNIST images from a grid in latent space\n",
    "#plot_results((encoder,decoder),\n",
    "#            (x_test,y_test),\n",
    "#                 batch_size=batch_size,\n",
    "#                 model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
